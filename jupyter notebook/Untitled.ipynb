{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全连接神经网络进行鸢尾花分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import *\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.57735448, -1.32923192, -1.18540157, -0.29662364],\n",
       "        [ 0.13115641, -1.15647866,  0.68250921, -1.06701086],\n",
       "        [ 0.60609368,  0.15276936,  1.59648582,  0.77553312],\n",
       "        ...,\n",
       "        [ 0.12081775, -2.11301695,  0.92555748,  0.3152947 ],\n",
       "        [ 0.94552668,  0.86050877,  2.31428667,  0.52105849],\n",
       "        [-0.52171284, -1.16674083, -1.08090372,  1.14649076]]),\n",
       " array([1, 0, 2, 0, 0, 2, 2, 0, 2, 0, 1, 2, 2, 1, 0, 2, 1, 0, 0, 0, 0, 2,\n",
       "        2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 1, 0, 2, 2, 1, 0, 1, 2, 2, 1, 0, 2,\n",
       "        2, 1, 0, 1, 2, 0, 0, 1, 2, 0, 2, 1, 0, 2, 2, 1, 0, 2, 0, 1, 2, 1,\n",
       "        0, 1, 0, 1, 1, 0, 0, 0, 2, 0, 2, 1, 2, 0, 2, 1, 1, 0, 1, 2, 1, 0,\n",
       "        1, 0, 0, 1, 1, 2, 1, 2, 1, 1, 1, 0, 1, 2, 0, 0, 2, 2, 1, 0, 1, 2,\n",
       "        0, 1, 1, 1, 1, 2, 2, 1, 2, 0, 2, 2, 0, 2, 1, 2, 1, 2, 1, 0, 0, 2,\n",
       "        0, 2, 2, 0, 2, 0, 1, 2, 1, 0, 0, 0, 2, 0, 0, 0, 1, 0, 2, 2, 1, 2,\n",
       "        1, 0, 1, 1, 2, 2, 0, 1, 1, 1, 0, 0, 2, 2, 1, 0, 0, 1, 0, 0, 2, 1,\n",
       "        0, 2, 2, 0, 2, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 2, 1, 0, 1, 1, 0, 2,\n",
       "        1, 0, 2, 0, 0, 1, 2, 2, 2, 2, 0, 2, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0,\n",
       "        2, 1, 2, 1, 2, 1, 2, 0, 0, 2, 1, 1, 2, 0, 2, 0, 0, 0, 2, 2, 2, 0,\n",
       "        1, 0, 0, 1, 0, 2, 0, 1, 2, 0, 0, 1, 1, 1, 1, 2, 0, 0, 2, 1, 0, 1,\n",
       "        0, 0, 1, 2, 2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 1, 0, 1, 1, 1, 2, 0, 1,\n",
       "        0, 0, 2, 2, 2, 1, 0, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 0, 0,\n",
       "        1, 1, 2, 0, 0, 0, 2, 2, 1, 1, 1, 2, 2, 0, 0, 0, 2, 2, 0, 2, 2, 0,\n",
       "        2, 1, 2, 0, 1, 1, 0, 1, 0, 1, 1, 2, 1, 0, 0, 0, 0, 2, 2, 2, 2, 1,\n",
       "        2, 2, 2, 0, 1, 0, 2, 2, 1, 2, 0, 0, 1, 0, 2, 1, 1, 1, 1, 0, 1, 1,\n",
       "        2, 1, 0, 1, 1, 2, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "        2, 2, 2, 0, 2, 1, 0, 0, 0, 1, 2, 2, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        0, 2, 2, 0, 2, 0, 1, 2, 0, 1, 0, 1, 2, 1, 2, 2, 0, 2, 2, 2, 0, 2,\n",
       "        2, 2, 2, 2, 0, 2, 2, 2, 0, 1, 0, 1, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1,\n",
       "        1, 1, 1, 2, 1, 0, 1, 2, 0, 0, 2, 1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "        0, 1, 2, 2, 0, 2, 1, 1, 2, 0, 1, 2, 2, 0, 0, 1]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSet = make_classification(n_classes=3, n_features=4, n_clusters_per_class=1, n_samples=500, random_state=2018, n_redundant=1, shuffle=1)\n",
    "dataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用PCA降到2维看看数据分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       ...,\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_y = LabelBinarizer().fit_transform(dataSet[1])\n",
    "raw_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(dataSet[0], raw_y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_layer(in_data, num_input, num_output, active_function=None):   #num_input:特征数, num_output:神经元数\n",
    "    weights = tf.Variable(tf.random_normal((num_input, num_output)))\n",
    "    bias = tf.Variable(tf.zeros((1, num_output)) + 0.5)\n",
    "    result = tf.matmul(in_data, weights) + bias\n",
    "    \n",
    "    if active_function == None:\n",
    "        return result\n",
    "    else:\n",
    "        return active_function(result)    #节点激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'add_layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-1fa97e7f5aba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnum_feat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0minput_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_feat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlay_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madd_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'add_layer' is not defined"
     ]
    }
   ],
   "source": [
    "num_feat = len(train_x[0])\n",
    "input_x = tf.placeholder(tf.float32, [None, num_feat])\n",
    "lay_0 = add_layer(input_x, 4, 10)\n",
    "lay_pred = add_layer(lay_0, 10, 3, tf.nn.relu)\n",
    "\n",
    "loss = tf.reduce_mean(tf.reduce_sum(abs(input_x - lay_pred), reduction_indices=1))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.08).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
